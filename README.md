# Data Platform for Finance Transactions

## ğŸ§­ Overview
This project implements an **on-premise data platform** built on the **Hadoop ecosystem** to perform **financial transaction analytics**.  
It follows a **Medallion Architecture (Bronze â†’ Silver â†’ Gold)** for data organization and integrates open-source technologies for ingestion, transformation, orchestration, and visualization.

---

## ğŸ—ï¸ Data Architecture

![Data Architecture](image.png)

### ğŸ”¹ Layer Overview

| Layer | Technologies | Description |
|-------|---------------|--------------|
| **Data Source** | PostgreSQL, JSON | The dataset is sourced from [Kaggle Transactions Fraud Dataset](https://www.kaggle.com/datasets/computingvictor/transactions-fraud-datasets). Data is bulk-loaded into PostgreSQL databases and simulated daily to generate new transaction records. |
| **Ingestion** | Debezium, Kafka, Spark | Debezium captures **Change Data Capture (CDC)** events from PostgreSQL (in progress). Kafka streams these CDC messages, while Spark ingests data from both databases and JSON files into the **data lake**. |
| **Data Lake** | Hadoop (HDFS), Hive, Spark, Trino | HDFS provides distributed storage; Hive stores metadata. Spark serves as the main compute engine for transformation. Trino enables unified SQL queries across the Bronze, Silver, and Gold layers. |
| **Serving & Visualization** | Trino, Apache Superset | Trino integrates with Superset to power interactive **finance analytics dashboards**. DBeaver is used for ad-hoc querying. |
| **Orchestration** | Apache Airflow | Airflow automates and schedules ingestion and transformation pipelines. |

---

## ğŸ“‚ Project Structure
```
F-DATA-PLATFORM/
â”‚
â”œâ”€â”€ data/ # Local data samples or exported datasets
â”œâ”€â”€ services/ # Service configurations (e.g., Docker, environment)
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ airflow/dags/ # Airflow DAGs for orchestration
â”‚ â”‚ â””â”€â”€ dl_dag.py
â”‚ â”‚
â”‚ â”œâ”€â”€ ingest/ # Batch ingestion scripts
â”‚ â”‚ â”œâ”€â”€ ingest.py
â”‚ â”‚ â”œâ”€â”€ mcc-codes.py
â”‚ â”‚ â””â”€â”€ train_fraud_labels.py
â”‚ â”‚
â”‚ â”œâ”€â”€ kafka/ # CDC consumers and Kafka integration
â”‚ â”‚ â”œâ”€â”€ cdc_cards.py
â”‚ â”‚ â”œâ”€â”€ cdc_transactions.py
â”‚ â”‚ â””â”€â”€ cdc_users.py
â”‚ â”‚
â”‚ â”œâ”€â”€ model/ # Data schema in Data Lake
â”‚ â”‚
â”‚ â”œâ”€â”€ transform/ # Spark transformations and aggregation scripts
â”‚ â”‚ â”‚ â”œâ”€â”€ cards.py
â”‚ â”‚ â”‚ â”œâ”€â”€ market_daily_summary.py
â”‚ â”‚ â”‚ â”œâ”€â”€ merchant_performance.py
â”‚ â”‚ â”‚ â”œâ”€â”€ transaction_details.py
â”‚ â”‚ â”‚ â”œâ”€â”€ transactions.py
â”‚ â”‚ â”‚ â”œâ”€â”€ user_detail.py
â”‚ â”‚ â”‚ â””â”€â”€ users.py
â”‚ â”‚ â””â”€â”€ trino_superset/
â”‚ â”‚ â””â”€â”€ view.sql
â”‚ â”‚
â”‚ â””â”€â”€ utils/ # Shared helper functions and configs
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ image.png # Data Architecture diagram
â””â”€â”€ README.md
```

---

## âš™ï¸ Data Flow

1. **Ingestion Phase**
   - Debezium streams changes from PostgreSQL tables (`users`, `cards`, `transactions`) to Kafka topics ***(In progress)***
   - Spark reads CDC streams or full table dumps and writes them into the **Bronze** layer of the Data Lake.

2. **Transformation Phase**
   - Spark processes and enriches the data from Bronze â†’ Silver â†’ Gold layers.
   - Scripts under `src/transform/temp` handle metrics like:
     - Market daily summary  
     - Merchant performance  
     - Transaction and user details  

3. **Storage and Metadata**
   - Data stored in **Parquet** format on HDFS.
   - Hive manages schema and partition metadata for all tables.

4. **Serving & Visualization**
   - Trino queries Hive tables across data layers.
   - Superset dashboards visualize KPIs, fraud detection patterns, and transaction trends. ***(In progress)***

5. **Orchestration**
   - Airflow DAGs coordinate ingestion and transformation pipelines on a schedule ***(In progress)***

---

## ğŸ§  Future Enhancements
- Add streaming transformation jobs using **Spark Structured Streaming**  
- Integrate **data quality checks**
- Using Table format like iceberg, upgrade to Data Lakehouse

---

## ğŸ“„ License
This project is released under the **MIT License**.